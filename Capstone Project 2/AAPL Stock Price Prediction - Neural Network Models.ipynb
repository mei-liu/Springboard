{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>DCLRDT</th>\n",
       "      <th>DIVAMT</th>\n",
       "      <th>PRC</th>\n",
       "      <th>VOL</th>\n",
       "      <th>OPENPRC</th>\n",
       "      <th>NUMTRD</th>\n",
       "      <th>sprtrn</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>...</th>\n",
       "      <th>SPRD_moving_max_10</th>\n",
       "      <th>SPRD_moving_max_15</th>\n",
       "      <th>SPRD_moving_max_50</th>\n",
       "      <th>OCdiff_moving_max_5</th>\n",
       "      <th>OCdiff_moving_max_10</th>\n",
       "      <th>OCdiff_moving_max_15</th>\n",
       "      <th>OCdiff_moving_max_50</th>\n",
       "      <th>PRC_week_encode</th>\n",
       "      <th>PRC_month_encode</th>\n",
       "      <th>PRC_year_encode</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1505</th>\n",
       "      <td>2018-12-24</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>146.83000</td>\n",
       "      <td>37249292</td>\n",
       "      <td>148.14999</td>\n",
       "      <td>253083</td>\n",
       "      <td>-0.027112</td>\n",
       "      <td>Monday</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.12001</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.10001</td>\n",
       "      <td>117.691859</td>\n",
       "      <td>121.716203</td>\n",
       "      <td>190.372739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506</th>\n",
       "      <td>2018-12-26</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>157.17000</td>\n",
       "      <td>58584634</td>\n",
       "      <td>148.30000</td>\n",
       "      <td>359309</td>\n",
       "      <td>0.049594</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.12001</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.10001</td>\n",
       "      <td>117.691859</td>\n",
       "      <td>121.716203</td>\n",
       "      <td>190.372739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1507</th>\n",
       "      <td>2018-12-27</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>156.14999</td>\n",
       "      <td>53117005</td>\n",
       "      <td>155.84000</td>\n",
       "      <td>335536</td>\n",
       "      <td>0.008563</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.12001</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.10001</td>\n",
       "      <td>117.691859</td>\n",
       "      <td>121.716203</td>\n",
       "      <td>190.372739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1508</th>\n",
       "      <td>2018-12-28</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>156.23000</td>\n",
       "      <td>42291347</td>\n",
       "      <td>157.50000</td>\n",
       "      <td>274455</td>\n",
       "      <td>-0.001242</td>\n",
       "      <td>Friday</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.12001</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.10001</td>\n",
       "      <td>117.691859</td>\n",
       "      <td>121.716203</td>\n",
       "      <td>190.372739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1509</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>157.74001</td>\n",
       "      <td>35003466</td>\n",
       "      <td>158.53000</td>\n",
       "      <td>207618</td>\n",
       "      <td>0.008492</td>\n",
       "      <td>Monday</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.12001</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.10001</td>\n",
       "      <td>110.597714</td>\n",
       "      <td>121.716203</td>\n",
       "      <td>190.372739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 125 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date  DCLRDT  DIVAMT        PRC       VOL    OPENPRC  NUMTRD  \\\n",
       "index                                                                       \n",
       "1505   2018-12-24       0     0.0  146.83000  37249292  148.14999  253083   \n",
       "1506   2018-12-26       0     0.0  157.17000  58584634  148.30000  359309   \n",
       "1507   2018-12-27       0     0.0  156.14999  53117005  155.84000  335536   \n",
       "1508   2018-12-28       0     0.0  156.23000  42291347  157.50000  274455   \n",
       "1509   2018-12-31       0     0.0  157.74001  35003466  158.53000  207618   \n",
       "\n",
       "         sprtrn        day  month  ...  SPRD_moving_max_10  \\\n",
       "index                              ...                       \n",
       "1505  -0.027112     Monday     12  ...                0.05   \n",
       "1506   0.049594  Wednesday     12  ...                0.05   \n",
       "1507   0.008563   Thursday     12  ...                0.05   \n",
       "1508  -0.001242     Friday     12  ...                0.05   \n",
       "1509   0.008492     Monday     12  ...                0.05   \n",
       "\n",
       "       SPRD_moving_max_15  SPRD_moving_max_50  OCdiff_moving_max_5  \\\n",
       "index                                                                \n",
       "1505                 0.05             0.12001                  5.0   \n",
       "1506                 0.05             0.12001                  5.0   \n",
       "1507                 0.05             0.12001                  5.0   \n",
       "1508                 0.05             0.12001                  5.0   \n",
       "1509                 0.05             0.12001                  5.0   \n",
       "\n",
       "       OCdiff_moving_max_10  OCdiff_moving_max_15  OCdiff_moving_max_50  \\\n",
       "index                                                                     \n",
       "1505                    5.0                   5.0               9.10001   \n",
       "1506                    5.0                   5.0               9.10001   \n",
       "1507                    5.0                   5.0               9.10001   \n",
       "1508                    5.0                   5.0               9.10001   \n",
       "1509                    5.0                   5.0               9.10001   \n",
       "\n",
       "       PRC_week_encode  PRC_month_encode  PRC_year_encode  \n",
       "index                                                      \n",
       "1505        117.691859        121.716203       190.372739  \n",
       "1506        117.691859        121.716203       190.372739  \n",
       "1507        117.691859        121.716203       190.372739  \n",
       "1508        117.691859        121.716203       190.372739  \n",
       "1509        110.597714        121.716203       190.372739  \n",
       "\n",
       "[5 rows x 125 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data/features (refer to Capstone 1 XGBoost model w/ multiple features for feature creation)\n",
    "train = pd.read_csv('/Users/meiliu/Documents/SpringBoard Related/AAPL_train.csv', index_col=0)\n",
    "test = pd.read_csv('/Users/meiliu/Documents/SpringBoard Related/AAPL_test.csv', index_col=0)\n",
    "\n",
    "data = pd.concat([train, test], axis=0)\n",
    "data.set_index('index', inplace=True)\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple MultiStep LSTM Model Using Only Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1430,) (1430,) (10,) (10,) (10,) (10,)\n"
     ]
    }
   ],
   "source": [
    "# split price data into train, validation, and test sets\n",
    "X_train_lstm1 = data['PRC'][:-30]\n",
    "y_train_lstm1 = data['PRC'][10:-20]\n",
    "\n",
    "X_val_lstm1 = data['PRC'][-30:-20]\n",
    "y_val_lstm1 = data['PRC'][-20:-10]\n",
    "\n",
    "X_test_lstm1 = data['PRC'][-20:-10]\n",
    "y_test_lstm1 = data['PRC'][-10:]\n",
    "\n",
    "print(X_train_lstm1.shape, y_train_lstm1.shape, X_val_lstm1.shape, y_val_lstm1.shape, \n",
    "      X_test_lstm1.shape, y_test_lstm1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape data to t=10 timesteps\n",
    "X_train_lstm1 = np.array(X_train_lstm1).reshape(int(len(X_train_lstm1)/10),10)\n",
    "y_train_lstm1 = np.array(y_train_lstm1).reshape(int(len(y_train_lstm1)/10),10)\n",
    "\n",
    "X_val_lstm1 = np.array(X_val_lstm1).reshape(int(len(X_val_lstm1)/10),10)\n",
    "y_val_lstm1 = np.array(y_val_lstm1).reshape(int(len(y_val_lstm1)/10),10)\n",
    "\n",
    "X_test_lstm1 = np.array(X_test_lstm1).reshape(1,10)\n",
    "y_test_lstm1 = np.array(y_test_lstm1).reshape(1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data using MinMax\n",
    "s = MinMaxScaler()\n",
    "X_train_lstm1 = s.fit_transform(X_train_lstm1)\n",
    "y_train_lstm1 = s.transform(y_train_lstm1)\n",
    "\n",
    "X_val_lstm1 = s.transform(X_val_lstm1)\n",
    "y_val_lstm1 = s.transform(y_val_lstm1)\n",
    "\n",
    "X_test_lstm1 = s.transform(X_test_lstm1)\n",
    "y_test_lstm1 = s.transform(y_test_lstm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X arrays to be 3D for LSTM layer\n",
    "X_train_lstm1 = np.array(X_train_lstm1).reshape(len(X_train_lstm1),10,1)\n",
    "X_val_lstm1 = np.array(X_val_lstm1).reshape(len(X_val_lstm1),10,1)\n",
    "X_test_lstm1 = np.array(X_test_lstm1).reshape(1,10,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model with single LSTM layer, 10 output nodes\n",
    "def priceLSTM():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=10, activation='relu', input_shape=(10,1)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143 samples, validate on 1 samples\n",
      "Epoch 1/100\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 0.2591 - val_loss: 0.5785\n",
      "Epoch 2/100\n",
      "143/143 [==============================] - 0s 532us/step - loss: 0.2468 - val_loss: 0.5545\n",
      "Epoch 3/100\n",
      "143/143 [==============================] - 0s 406us/step - loss: 0.2395 - val_loss: 0.5331\n",
      "Epoch 4/100\n",
      "143/143 [==============================] - 0s 460us/step - loss: 0.2257 - val_loss: 0.5147\n",
      "Epoch 5/100\n",
      "143/143 [==============================] - 0s 497us/step - loss: 0.2193 - val_loss: 0.4983\n",
      "Epoch 6/100\n",
      "143/143 [==============================] - 0s 397us/step - loss: 0.2105 - val_loss: 0.4835\n",
      "Epoch 7/100\n",
      "143/143 [==============================] - 0s 479us/step - loss: 0.2043 - val_loss: 0.4702\n",
      "Epoch 8/100\n",
      "143/143 [==============================] - 0s 473us/step - loss: 0.1980 - val_loss: 0.4580\n",
      "Epoch 9/100\n",
      "143/143 [==============================] - 0s 420us/step - loss: 0.1920 - val_loss: 0.4467\n",
      "Epoch 10/100\n",
      "143/143 [==============================] - 0s 385us/step - loss: 0.1867 - val_loss: 0.4363\n",
      "Epoch 11/100\n",
      "143/143 [==============================] - 0s 395us/step - loss: 0.1824 - val_loss: 0.4264\n",
      "Epoch 12/100\n",
      "143/143 [==============================] - 0s 395us/step - loss: 0.1773 - val_loss: 0.4172\n",
      "Epoch 13/100\n",
      "143/143 [==============================] - 0s 395us/step - loss: 0.1723 - val_loss: 0.4087\n",
      "Epoch 14/100\n",
      "143/143 [==============================] - 0s 375us/step - loss: 0.1685 - val_loss: 0.4000\n",
      "Epoch 15/100\n",
      "143/143 [==============================] - 0s 380us/step - loss: 0.1628 - val_loss: 0.3886\n",
      "Epoch 16/100\n",
      "143/143 [==============================] - 0s 363us/step - loss: 0.1583 - val_loss: 0.3772\n",
      "Epoch 17/100\n",
      "143/143 [==============================] - 0s 437us/step - loss: 0.1528 - val_loss: 0.3656\n",
      "Epoch 18/100\n",
      "143/143 [==============================] - 0s 470us/step - loss: 0.1477 - val_loss: 0.3534\n",
      "Epoch 19/100\n",
      "143/143 [==============================] - 0s 441us/step - loss: 0.1424 - val_loss: 0.3412\n",
      "Epoch 20/100\n",
      "143/143 [==============================] - 0s 401us/step - loss: 0.1356 - val_loss: 0.3287\n",
      "Epoch 21/100\n",
      "143/143 [==============================] - 0s 395us/step - loss: 0.1311 - val_loss: 0.3160\n",
      "Epoch 22/100\n",
      "143/143 [==============================] - 0s 460us/step - loss: 0.1267 - val_loss: 0.3026\n",
      "Epoch 23/100\n",
      "143/143 [==============================] - 0s 389us/step - loss: 0.1198 - val_loss: 0.2886\n",
      "Epoch 24/100\n",
      "143/143 [==============================] - 0s 432us/step - loss: 0.1155 - val_loss: 0.2739\n",
      "Epoch 25/100\n",
      "143/143 [==============================] - 0s 374us/step - loss: 0.1115 - val_loss: 0.2588\n",
      "Epoch 26/100\n",
      "143/143 [==============================] - 0s 382us/step - loss: 0.1037 - val_loss: 0.2435\n",
      "Epoch 27/100\n",
      "143/143 [==============================] - 0s 455us/step - loss: 0.0958 - val_loss: 0.2269\n",
      "Epoch 28/100\n",
      "143/143 [==============================] - 0s 525us/step - loss: 0.0928 - val_loss: 0.2104\n",
      "Epoch 29/100\n",
      "143/143 [==============================] - 0s 417us/step - loss: 0.0847 - val_loss: 0.1936\n",
      "Epoch 30/100\n",
      "143/143 [==============================] - 0s 429us/step - loss: 0.0873 - val_loss: 0.1768\n",
      "Epoch 31/100\n",
      "143/143 [==============================] - 0s 377us/step - loss: 0.0804 - val_loss: 0.1607\n",
      "Epoch 32/100\n",
      "143/143 [==============================] - 0s 405us/step - loss: 0.0679 - val_loss: 0.1451\n",
      "Epoch 33/100\n",
      "143/143 [==============================] - 0s 397us/step - loss: 0.0639 - val_loss: 0.1309\n",
      "Epoch 34/100\n",
      "143/143 [==============================] - 0s 375us/step - loss: 0.0618 - val_loss: 0.1159\n",
      "Epoch 35/100\n",
      "143/143 [==============================] - 0s 379us/step - loss: 0.0624 - val_loss: 0.1041\n",
      "Epoch 36/100\n",
      "143/143 [==============================] - 0s 596us/step - loss: 0.0556 - val_loss: 0.0941\n",
      "Epoch 37/100\n",
      "143/143 [==============================] - 0s 391us/step - loss: 0.0568 - val_loss: 0.0860\n",
      "Epoch 38/100\n",
      "143/143 [==============================] - 0s 397us/step - loss: 0.0389 - val_loss: 0.0785\n",
      "Epoch 39/100\n",
      "143/143 [==============================] - 0s 398us/step - loss: 0.0416 - val_loss: 0.0718\n",
      "Epoch 40/100\n",
      "143/143 [==============================] - 0s 386us/step - loss: 0.0542 - val_loss: 0.0647\n",
      "Epoch 41/100\n",
      "143/143 [==============================] - 0s 391us/step - loss: 0.0491 - val_loss: 0.0580\n",
      "Epoch 42/100\n",
      "143/143 [==============================] - 0s 427us/step - loss: 0.0405 - val_loss: 0.0538\n",
      "Epoch 43/100\n",
      "143/143 [==============================] - 0s 396us/step - loss: 0.0424 - val_loss: 0.0507\n",
      "Epoch 44/100\n",
      "143/143 [==============================] - 0s 431us/step - loss: 0.0383 - val_loss: 0.0470\n",
      "Epoch 45/100\n",
      "143/143 [==============================] - 0s 409us/step - loss: 0.0445 - val_loss: 0.0426\n",
      "Epoch 46/100\n",
      "143/143 [==============================] - 0s 393us/step - loss: 0.0281 - val_loss: 0.0380\n",
      "Epoch 47/100\n",
      "143/143 [==============================] - 0s 455us/step - loss: 0.0331 - val_loss: 0.0339\n",
      "Epoch 48/100\n",
      "143/143 [==============================] - 0s 423us/step - loss: 0.0248 - val_loss: 0.0319\n",
      "Epoch 49/100\n",
      "143/143 [==============================] - 0s 396us/step - loss: 0.0274 - val_loss: 0.0288\n",
      "Epoch 50/100\n",
      "143/143 [==============================] - 0s 376us/step - loss: 0.0327 - val_loss: 0.0262\n",
      "Epoch 51/100\n",
      "143/143 [==============================] - 0s 414us/step - loss: 0.0321 - val_loss: 0.0234\n",
      "Epoch 52/100\n",
      "143/143 [==============================] - 0s 433us/step - loss: 0.0344 - val_loss: 0.0204\n",
      "Epoch 53/100\n",
      "143/143 [==============================] - 0s 472us/step - loss: 0.0247 - val_loss: 0.0189\n",
      "Epoch 54/100\n",
      "143/143 [==============================] - 0s 577us/step - loss: 0.0236 - val_loss: 0.0176\n",
      "Epoch 55/100\n",
      "143/143 [==============================] - 0s 394us/step - loss: 0.0272 - val_loss: 0.0160\n",
      "Epoch 56/100\n",
      "143/143 [==============================] - 0s 385us/step - loss: 0.0263 - val_loss: 0.0137\n",
      "Epoch 57/100\n",
      "143/143 [==============================] - 0s 432us/step - loss: 0.0261 - val_loss: 0.0113\n",
      "Epoch 58/100\n",
      "143/143 [==============================] - 0s 373us/step - loss: 0.0219 - val_loss: 0.0113\n",
      "Epoch 59/100\n",
      "143/143 [==============================] - 0s 384us/step - loss: 0.0260 - val_loss: 0.0110\n",
      "Epoch 60/100\n",
      "143/143 [==============================] - 0s 339us/step - loss: 0.0207 - val_loss: 0.0098\n",
      "Epoch 61/100\n",
      "143/143 [==============================] - 0s 392us/step - loss: 0.0217 - val_loss: 0.0078\n",
      "Epoch 62/100\n",
      "143/143 [==============================] - 0s 403us/step - loss: 0.0195 - val_loss: 0.0065\n",
      "Epoch 63/100\n",
      "143/143 [==============================] - 0s 452us/step - loss: 0.0243 - val_loss: 0.0063\n",
      "Epoch 64/100\n",
      "143/143 [==============================] - 0s 387us/step - loss: 0.0206 - val_loss: 0.0062\n",
      "Epoch 65/100\n",
      "143/143 [==============================] - 0s 372us/step - loss: 0.0154 - val_loss: 0.0062\n",
      "Epoch 66/100\n",
      "143/143 [==============================] - 0s 410us/step - loss: 0.0166 - val_loss: 0.0075\n",
      "Epoch 67/100\n",
      "143/143 [==============================] - 0s 469us/step - loss: 0.0158 - val_loss: 0.0057\n",
      "Epoch 68/100\n",
      "143/143 [==============================] - 0s 413us/step - loss: 0.0160 - val_loss: 0.0046\n",
      "Epoch 69/100\n",
      "143/143 [==============================] - 0s 486us/step - loss: 0.0131 - val_loss: 0.0049\n",
      "Epoch 70/100\n",
      "143/143 [==============================] - 0s 523us/step - loss: 0.0173 - val_loss: 0.0042\n",
      "Epoch 71/100\n",
      "143/143 [==============================] - 0s 394us/step - loss: 0.0151 - val_loss: 0.0039\n",
      "Epoch 72/100\n",
      "143/143 [==============================] - 0s 439us/step - loss: 0.0158 - val_loss: 0.0029\n",
      "Epoch 73/100\n",
      "143/143 [==============================] - 0s 370us/step - loss: 0.0167 - val_loss: 0.0022\n",
      "Epoch 74/100\n",
      "143/143 [==============================] - 0s 379us/step - loss: 0.0217 - val_loss: 0.0019\n",
      "Epoch 75/100\n",
      "143/143 [==============================] - 0s 484us/step - loss: 0.0192 - val_loss: 0.0023\n",
      "Epoch 76/100\n",
      "143/143 [==============================] - 0s 425us/step - loss: 0.0137 - val_loss: 0.0024\n",
      "Epoch 77/100\n",
      "143/143 [==============================] - 0s 435us/step - loss: 0.0178 - val_loss: 0.0020\n",
      "Epoch 78/100\n",
      "143/143 [==============================] - 0s 456us/step - loss: 0.0135 - val_loss: 0.0027\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143/143 [==============================] - 0s 433us/step - loss: 0.0142 - val_loss: 0.0029\n",
      "Epoch 80/100\n",
      "143/143 [==============================] - 0s 447us/step - loss: 0.0152 - val_loss: 0.0029\n",
      "Epoch 81/100\n",
      "143/143 [==============================] - 0s 396us/step - loss: 0.0142 - val_loss: 0.0023\n",
      "Epoch 82/100\n",
      "143/143 [==============================] - 0s 525us/step - loss: 0.0161 - val_loss: 0.0018\n",
      "Epoch 83/100\n",
      "143/143 [==============================] - 0s 374us/step - loss: 0.0104 - val_loss: 0.0014\n",
      "Epoch 84/100\n",
      "143/143 [==============================] - 0s 416us/step - loss: 0.0151 - val_loss: 0.0021\n",
      "Epoch 85/100\n",
      "143/143 [==============================] - 0s 374us/step - loss: 0.0157 - val_loss: 0.0015\n",
      "Epoch 86/100\n",
      "143/143 [==============================] - 0s 399us/step - loss: 0.0141 - val_loss: 0.0011\n",
      "Epoch 87/100\n",
      "143/143 [==============================] - 0s 367us/step - loss: 0.0123 - val_loss: 0.0011\n",
      "Epoch 88/100\n",
      "143/143 [==============================] - 0s 376us/step - loss: 0.0170 - val_loss: 0.0016\n",
      "Epoch 89/100\n",
      "143/143 [==============================] - 0s 450us/step - loss: 0.0114 - val_loss: 0.0026\n",
      "Epoch 90/100\n",
      "143/143 [==============================] - 0s 379us/step - loss: 0.0127 - val_loss: 0.0032\n",
      "Epoch 91/100\n",
      "143/143 [==============================] - 0s 375us/step - loss: 0.0165 - val_loss: 0.0023\n",
      "Epoch 92/100\n",
      "143/143 [==============================] - 0s 414us/step - loss: 0.0095 - val_loss: 0.0023\n",
      "Epoch 93/100\n",
      "143/143 [==============================] - 0s 379us/step - loss: 0.0096 - val_loss: 0.0028\n",
      "Epoch 94/100\n",
      "143/143 [==============================] - 0s 375us/step - loss: 0.0095 - val_loss: 0.0017\n",
      "Epoch 95/100\n",
      "143/143 [==============================] - 0s 401us/step - loss: 0.0142 - val_loss: 0.0015\n",
      "Epoch 96/100\n",
      "143/143 [==============================] - 0s 407us/step - loss: 0.0133 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00096: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 97/100\n",
      "143/143 [==============================] - 0s 394us/step - loss: 0.0104 - val_loss: 0.0013\n",
      "Epoch 98/100\n",
      "143/143 [==============================] - 0s 632us/step - loss: 0.0140 - val_loss: 0.0014\n",
      "Epoch 99/100\n",
      "143/143 [==============================] - 0s 396us/step - loss: 0.0092 - val_loss: 0.0015\n",
      "Epoch 100/100\n",
      "143/143 [==============================] - 0s 393us/step - loss: 0.0098 - val_loss: 0.0016\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a5ae12f60>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model = priceLSTM()\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=1, mode='auto', cooldown=0, min_lr=0.0001)\n",
    "checkpoint = ModelCheckpoint('best_model_weights.hdf5',monitor='val_loss',save_best_only=True)\n",
    "early_stopping_monitor = EarlyStopping(patience=30)\n",
    "\n",
    "model.fit(x=X_train_lstm1, y=y_train_lstm1, validation_data=(X_val_lstm1, y_val_lstm1), epochs=100,\n",
    "          callbacks=[lr_reducer, checkpoint, early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict test set\n",
    "y_pred_lstm1 = model.predict(X_test_lstm1)\n",
    "y_pred_lstm1 = s.inverse_transform(y_pred_lstm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE calculation function\n",
    "def rmse(pred, target):\n",
    "    return np.sqrt(((pred - target) ** 2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM w/ MinMax scaled Price:\n",
      "[[162.42427 164.53387 165.8459  159.88469 162.28246 161.48015 160.19829\n",
      "  163.46373 166.03085 160.99812]] \n",
      "-----\n",
      "[[163.94    166.07001 160.89    156.83    150.73    146.83    157.17\n",
      "  156.14999 156.23    157.74001]] \n",
      "-----\n",
      "rmse: 7.456330543095471\n"
     ]
    }
   ],
   "source": [
    "print('LSTM w/ MinMax scaled Price:')\n",
    "print(y_pred_lstm1, '\\n-----')\n",
    "print(s.inverse_transform(y_test_lstm1), '\\n-----')\n",
    "print('rmse:', rmse(y_pred_lstm1,s.inverse_transform(y_test_lstm1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions and prediction accuracy are quite different across runs of the model. I'll use the average prediction RMSE across 30 different runs to estimate the RMSE of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTMfit(n_iter):\n",
    "    rmse_list = []\n",
    "    for i in range(n_iter):\n",
    "        model = priceLSTM()\n",
    "        lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto', cooldown=0, min_lr=0.0001)\n",
    "        checkpoint = ModelCheckpoint('best_model_weights.hdf5',monitor='val_loss',save_best_only=True)\n",
    "        early_stopping_monitor = EarlyStopping(patience=30)\n",
    "\n",
    "        model.fit(x=X_train_lstm1, y=y_train_lstm1, validation_data=(X_val_lstm1, y_val_lstm1), epochs=100,\n",
    "              callbacks=[lr_reducer, checkpoint, early_stopping_monitor], verbose=0)\n",
    "        y_pred_lstm1 = s.inverse_transform(model.predict(X_test_lstm1))\n",
    "        rmse_list.append(rmse(y_pred_lstm1,s.inverse_transform(y_test_lstm1)))\n",
    "    return np.mean(np.array(rmse_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg RMSE across 30 iterations:  7.657567875090273\n"
     ]
    }
   ],
   "source": [
    "avg_rmse = LSTMfit(30)\n",
    "print('Avg RMSE across 30 iterations: ', avg_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Sequential Model w/ Categorical Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the target, categorical features, and continuous features\n",
    "target = ['PRC']\n",
    "cat_vars = ['DCLRDT', 'month','year','dayofweek', 'qtr', 'day365', 'dayofmonth',\n",
    "            'weekofyear', 'startend','Announce']\n",
    "\n",
    "drop=['date','VOL','OPENPRC','NUMTRD','sprtrn','day','HISPRD','SPRD', 'OCdiff']\n",
    "\n",
    "cont_vars = list(data.columns)\n",
    "\n",
    "cont_vars.remove(target[0])\n",
    "\n",
    "for cat in cat_vars:\n",
    "    cont_vars.remove(cat)\n",
    "\n",
    "for d in drop:\n",
    "    cont_vars.remove(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCLRDT: [0 1]\n",
      "month: [ 1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "year: [2013 2014 2015 2016 2017 2018]\n",
      "dayofweek: [0 1 2 3 4]\n",
      "qtr: [1 2 3 4]\n",
      "day365: [  2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19\n",
      "  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37\n",
      "  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55\n",
      "  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73\n",
      "  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91\n",
      "  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109\n",
      " 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127\n",
      " 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145\n",
      " 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163\n",
      " 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181\n",
      " 182 183 184 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200\n",
      " 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218\n",
      " 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236\n",
      " 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254\n",
      " 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272\n",
      " 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290\n",
      " 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308\n",
      " 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326\n",
      " 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344\n",
      " 345 346 347 348 349 350 351 352 353 354 355 356 357 358 360 361 362 363\n",
      " 364 365]\n",
      "dayofmonth: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31]\n",
      "weekofyear: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53]\n",
      "startend: [0 1]\n",
      "Announce: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
      " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30.]\n"
     ]
    }
   ],
   "source": [
    "# Label encode each categorical feature\n",
    "encoders = {}\n",
    "for v in cat_vars:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(data[v].values)\n",
    "    encoders[v] = le\n",
    "    data.loc[:, v] = le.transform(data[v].values)\n",
    "    print('{0}: {1}'.format(v, le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training, validation, and test sets\n",
    "X = data[cat_vars + cont_vars][:-110].copy()\n",
    "y = data[target][:-110].copy()\n",
    "\n",
    "X_val = data[cat_vars + cont_vars][-110:-10].copy()\n",
    "y_val = data[target][-110:-10].copy()\n",
    "\n",
    "X_test = data[cat_vars + cont_vars][-10:].copy()\n",
    "y_test = data[target][-10:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale continuous variables using scaler fitted on only training data\n",
    "scaler = MinMaxScaler()\n",
    "X.loc[:, cont_vars] = scaler.fit_transform(X[cont_vars].values)\n",
    "X_val.loc[:, cont_vars] = scaler.transform(X_val[cont_vars].values)\n",
    "X_test.loc[:, cont_vars] = scaler.transform(X_test[cont_vars].values)\n",
    "\n",
    "# scale target by max target\n",
    "y_max = np.max(train['PRC'])\n",
    "y = y/y_max\n",
    "y_val = y_val/y_max\n",
    "y_test = y_test/y_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change categorical features to type category, continuous features to type float32\n",
    "for v in cat_vars:\n",
    "    X[v] = X[v].astype('int').astype('category').cat.as_ordered()\n",
    "    X_val[v] = X_val[v].astype('int').astype('category').cat.as_ordered()\n",
    "    X_test[v] = X_test[v].astype('int').astype('category').cat.as_ordered()\n",
    "for v in cont_vars:\n",
    "    X[v] = X[v].astype('float32')\n",
    "    X_val[v] = X_val[v].astype('float32')\n",
    "    X_test[v] = X_test[v].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1350 entries, 50 to 1399\n",
      "Columns: 115 entries, DCLRDT to PRC_year_encode\n",
      "dtypes: category(10), float32(105)\n",
      "memory usage: 598.7 KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1350, 115), (100, 115), (10, 115), (1350, 1), (100, 1), (10, 1))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the shape of each input\n",
    "print(X.info())\n",
    "\n",
    "X.shape, X_val.shape, X_test.shape, y.shape, y_val.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DCLRDT', 2),\n",
       " ('month', 12),\n",
       " ('year', 6),\n",
       " ('dayofweek', 5),\n",
       " ('qtr', 4),\n",
       " ('day365', 362),\n",
       " ('dayofmonth', 31),\n",
       " ('weekofyear', 53),\n",
       " ('startend', 2),\n",
       " ('Announce', 31)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input size of categorical variables\n",
    "cat_sizes = [(c, len(X[c].cat.categories)) for c in cat_vars]\n",
    "cat_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 1),\n",
       " (12, 6),\n",
       " (6, 3),\n",
       " (5, 3),\n",
       " (4, 2),\n",
       " (362, 50),\n",
       " (31, 16),\n",
       " (53, 27),\n",
       " (2, 1),\n",
       " (31, 16)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding size of categorical variables\n",
    "embedding_sizes = [(c, min(50, (c + 1) // 2)) for _, c in cat_sizes]\n",
    "embedding_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 DCLRDT\n",
      "1 month\n",
      "2 year\n",
      "3 dayofweek\n",
      "4 qtr\n",
      "5 day365\n",
      "6 dayofmonth\n",
      "7 weekofyear\n",
      "8 startend\n",
      "9 Announce\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11, 11, 11)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create input arrays to feed into the neural network\n",
    "X_arr = []\n",
    "X_val_arr = []\n",
    "X_test_arr = []\n",
    "\n",
    "for i, v in enumerate(cat_vars):\n",
    "    print(i,v)\n",
    "    X_arr.append(X.iloc[:, i])\n",
    "    X_val_arr.append(X_val.iloc[:, i])\n",
    "    X_test_arr.append(X_test.iloc[:, i])\n",
    "\n",
    "X_arr.append(X.iloc[:, len(cat_vars):])\n",
    "X_val_arr.append(X_val.iloc[:, len(cat_vars):])\n",
    "X_test_arr.append(X_test.iloc[:, len(cat_vars):])\n",
    "\n",
    "len(X_arr), len(X_val_arr), len(X_test_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Concatenate, Input, Reshape\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a sequential neural network with an embedding layer\n",
    "def EmbeddingNet(cat_vars, cont_vars, embedding_sizes):\n",
    "    inputs = []\n",
    "    embed_layers = []\n",
    "    for (c, (in_size, out_size)) in zip(cat_vars, embedding_sizes):\n",
    "        # define input layer\n",
    "        i = Input(shape=(1,))\n",
    "        #define embedding layer\n",
    "        o = Embedding(in_size, out_size, name=c)(i)\n",
    "        o = Reshape(target_shape=(out_size,))(o)\n",
    "        # store layers\n",
    "        inputs.append(i)\n",
    "        embed_layers.append(o)\n",
    "    \n",
    "    # concat embedding layers\n",
    "    embed = Concatenate()(embed_layers)\n",
    "    embed = Dropout(0.1)(embed)\n",
    "\n",
    "    cont_input = Input(shape=(len(cont_vars),))\n",
    "    inputs.append(cont_input)\n",
    "    \n",
    "    # concat embeddings and continuous variables to input into sequential nn\n",
    "    x = Concatenate()([embed, cont_input])\n",
    "\n",
    "    dense = Dense(1000, activation='relu', kernel_initializer='he_normal')(x)\n",
    "    dense = Dropout(0.1)(dense)\n",
    "    \n",
    "    dense = Dense(500, activation='relu', kernel_initializer='he_normal')(dense)\n",
    "    dense = Dropout(0.1)(dense)\n",
    "    \n",
    "    output = Dense(1, activation='sigmoid', kernel_initializer='he_normal')(dense)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/meiliu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1350 samples, validate on 100 samples\n",
      "Epoch 1/100\n",
      "1350/1350 [==============================] - 2s 1ms/step - loss: 0.0132 - val_loss: 0.0049\n",
      "Epoch 2/100\n",
      "1350/1350 [==============================] - 1s 835us/step - loss: 7.2193e-04 - val_loss: 0.0048\n",
      "Epoch 3/100\n",
      "1350/1350 [==============================] - 1s 695us/step - loss: 5.3182e-04 - val_loss: 0.0055\n",
      "Epoch 4/100\n",
      "1350/1350 [==============================] - 1s 729us/step - loss: 4.2026e-04 - val_loss: 0.0044\n",
      "Epoch 5/100\n",
      "1350/1350 [==============================] - 1s 797us/step - loss: 3.9723e-04 - val_loss: 0.0036\n",
      "Epoch 6/100\n",
      "1350/1350 [==============================] - 1s 763us/step - loss: 3.6457e-04 - val_loss: 0.0049\n",
      "Epoch 7/100\n",
      "1350/1350 [==============================] - 1s 905us/step - loss: 2.8579e-04 - val_loss: 0.0041\n",
      "Epoch 8/100\n",
      "1350/1350 [==============================] - 1s 752us/step - loss: 2.4107e-04 - val_loss: 0.0044\n",
      "Epoch 9/100\n",
      "1350/1350 [==============================] - 1s 729us/step - loss: 2.6079e-04 - val_loss: 0.0044\n",
      "Epoch 10/100\n",
      "1350/1350 [==============================] - 1s 701us/step - loss: 2.2336e-04 - val_loss: 0.0038\n",
      "Epoch 11/100\n",
      "1350/1350 [==============================] - 1s 727us/step - loss: 1.9692e-04 - val_loss: 0.0033\n",
      "Epoch 12/100\n",
      "1350/1350 [==============================] - 1s 730us/step - loss: 2.1840e-04 - val_loss: 0.0040\n",
      "Epoch 13/100\n",
      "1350/1350 [==============================] - 1s 684us/step - loss: 1.6197e-04 - val_loss: 0.0038\n",
      "Epoch 14/100\n",
      "1350/1350 [==============================] - 1s 847us/step - loss: 1.8613e-04 - val_loss: 0.0045\n",
      "Epoch 15/100\n",
      "1350/1350 [==============================] - 1s 777us/step - loss: 1.6243e-04 - val_loss: 0.0043\n",
      "Epoch 16/100\n",
      "1350/1350 [==============================] - 1s 759us/step - loss: 1.7913e-04 - val_loss: 0.0035\n",
      "Epoch 17/100\n",
      "1350/1350 [==============================] - 1s 669us/step - loss: 1.6703e-04 - val_loss: 0.0034\n",
      "Epoch 18/100\n",
      "1350/1350 [==============================] - 1s 794us/step - loss: 1.8574e-04 - val_loss: 0.0057\n",
      "Epoch 19/100\n",
      "1350/1350 [==============================] - 1s 768us/step - loss: 1.5357e-04 - val_loss: 0.0043\n",
      "Epoch 20/100\n",
      "1350/1350 [==============================] - 1s 728us/step - loss: 1.3236e-04 - val_loss: 0.0031\n",
      "Epoch 21/100\n",
      "1350/1350 [==============================] - 1s 681us/step - loss: 1.3265e-04 - val_loss: 0.0035\n",
      "Epoch 22/100\n",
      "1350/1350 [==============================] - 1s 695us/step - loss: 1.1646e-04 - val_loss: 0.0038\n",
      "Epoch 23/100\n",
      "1350/1350 [==============================] - 1s 713us/step - loss: 1.3695e-04 - val_loss: 0.0034\n",
      "Epoch 24/100\n",
      "1350/1350 [==============================] - 1s 723us/step - loss: 1.3672e-04 - val_loss: 0.0043\n",
      "Epoch 25/100\n",
      "1350/1350 [==============================] - 1s 807us/step - loss: 1.1662e-04 - val_loss: 0.0049\n",
      "Epoch 26/100\n",
      "1350/1350 [==============================] - 1s 845us/step - loss: 1.3488e-04 - val_loss: 0.0036\n",
      "Epoch 27/100\n",
      "1350/1350 [==============================] - 1s 685us/step - loss: 1.3439e-04 - val_loss: 0.0037\n",
      "Epoch 28/100\n",
      "1350/1350 [==============================] - 1s 719us/step - loss: 1.3232e-04 - val_loss: 0.0046\n",
      "Epoch 29/100\n",
      "1350/1350 [==============================] - 1s 703us/step - loss: 1.1535e-04 - val_loss: 0.0039\n",
      "Epoch 30/100\n",
      "1350/1350 [==============================] - 1s 672us/step - loss: 1.0997e-04 - val_loss: 0.0036\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 31/100\n",
      "1350/1350 [==============================] - 1s 795us/step - loss: 8.9385e-05 - val_loss: 0.0043\n",
      "Epoch 32/100\n",
      "1350/1350 [==============================] - 1s 790us/step - loss: 8.1792e-05 - val_loss: 0.0042\n",
      "Epoch 33/100\n",
      "1350/1350 [==============================] - 1s 995us/step - loss: 8.5146e-05 - val_loss: 0.0040\n",
      "Epoch 34/100\n",
      "1350/1350 [==============================] - 1s 814us/step - loss: 8.7002e-05 - val_loss: 0.0040\n",
      "Epoch 35/100\n",
      "1350/1350 [==============================] - 1s 997us/step - loss: 8.4116e-05 - val_loss: 0.0040\n",
      "Epoch 36/100\n",
      "1350/1350 [==============================] - 1s 848us/step - loss: 8.3065e-05 - val_loss: 0.0042\n",
      "Epoch 37/100\n",
      "1350/1350 [==============================] - 1s 840us/step - loss: 7.6921e-05 - val_loss: 0.0040\n",
      "Epoch 38/100\n",
      "1350/1350 [==============================] - 1s 718us/step - loss: 7.8020e-05 - val_loss: 0.0039\n",
      "Epoch 39/100\n",
      "1350/1350 [==============================] - 1s 739us/step - loss: 7.6049e-05 - val_loss: 0.0039\n",
      "Epoch 40/100\n",
      "1350/1350 [==============================] - 1s 757us/step - loss: 7.3458e-05 - val_loss: 0.0041\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0001.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a590ebf98>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize model\n",
    "model = EmbeddingNet(cat_vars, cont_vars, embedding_sizes)\n",
    "\n",
    "# fit model with training data and evaluate using validation data\n",
    "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=1, mode='auto', cooldown=0, min_lr=0.0001)\n",
    "checkpoint = ModelCheckpoint('best_model_weights.hdf5',monitor='val_loss',save_best_only=True)\n",
    "early_stopping_monitor = EarlyStopping(patience=20)\n",
    "\n",
    "model.fit(x=X_arr, y=y, validation_data=(X_val_arr, y_val), epochs=100,\n",
    "          callbacks=[lr_reducer, checkpoint, early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_56 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_57 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_58 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_59 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_60 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_61 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_62 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_63 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_64 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_65 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "DCLRDT (Embedding)              (None, 1, 1)         2           input_56[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "month (Embedding)               (None, 1, 6)         72          input_57[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "year (Embedding)                (None, 1, 3)         18          input_58[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dayofweek (Embedding)           (None, 1, 3)         15          input_59[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "qtr (Embedding)                 (None, 1, 2)         8           input_60[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "day365 (Embedding)              (None, 1, 50)        18100       input_61[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dayofmonth (Embedding)          (None, 1, 16)        496         input_62[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "weekofyear (Embedding)          (None, 1, 27)        1431        input_63[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "startend (Embedding)            (None, 1, 1)         2           input_64[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Announce (Embedding)            (None, 1, 16)        496         input_65[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_51 (Reshape)            (None, 1)            0           DCLRDT[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "reshape_52 (Reshape)            (None, 6)            0           month[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "reshape_53 (Reshape)            (None, 3)            0           year[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "reshape_54 (Reshape)            (None, 3)            0           dayofweek[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_55 (Reshape)            (None, 2)            0           qtr[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "reshape_56 (Reshape)            (None, 50)           0           day365[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "reshape_57 (Reshape)            (None, 16)           0           dayofmonth[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_58 (Reshape)            (None, 27)           0           weekofyear[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_59 (Reshape)            (None, 1)            0           startend[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_60 (Reshape)            (None, 16)           0           Announce[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 125)          0           reshape_51[0][0]                 \n",
      "                                                                 reshape_52[0][0]                 \n",
      "                                                                 reshape_53[0][0]                 \n",
      "                                                                 reshape_54[0][0]                 \n",
      "                                                                 reshape_55[0][0]                 \n",
      "                                                                 reshape_56[0][0]                 \n",
      "                                                                 reshape_57[0][0]                 \n",
      "                                                                 reshape_58[0][0]                 \n",
      "                                                                 reshape_59[0][0]                 \n",
      "                                                                 reshape_60[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_48 (Dropout)            (None, 125)          0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "input_66 (InputLayer)           (None, 105)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 230)          0           dropout_48[0][0]                 \n",
      "                                                                 input_66[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_48 (Dense)                (None, 1000)         231000      concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_49 (Dropout)            (None, 1000)         0           dense_48[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_49 (Dense)                (None, 500)          500500      dropout_49[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_50 (Dropout)            (None, 500)          0           dense_49[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_50 (Dense)                (None, 1)            501         dropout_50[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 752,641\n",
      "Trainable params: 752,641\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict test set\n",
    "pred = model.predict(x=X_test_arr)\n",
    "pred = pred*y_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cont. features scaled using MinMax, target scaled using y_max:\n",
      "[[174.14186]\n",
      " [173.86067]\n",
      " [175.03516]\n",
      " [173.1701 ]\n",
      " [172.79112]\n",
      " [172.00076]\n",
      " [171.49315]\n",
      " [171.90062]\n",
      " [173.58289]\n",
      " [171.47313]] \n",
      "-----\n",
      "             PRC\n",
      "index           \n",
      "1500   163.94000\n",
      "1501   166.07001\n",
      "1502   160.89000\n",
      "1503   156.83000\n",
      "1504   150.73000\n",
      "1505   146.83000\n",
      "1506   157.17000\n",
      "1507   156.14999\n",
      "1508   156.23000\n",
      "1509   157.74001 \n",
      "-----\n",
      "rmse: PRC    16.416679\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('cont. features scaled using MinMax, target scaled using y_max:')\n",
    "print(pred, '\\n-----')\n",
    "print(y_test*y_max, '\\n-----')\n",
    "print('rmse:', rmse(pred,y_test*y_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "def EmbedNetFit(n_iter):\n",
    "    rmse_list = []\n",
    "    for i in range(n_iter):\n",
    "        model = EmbeddingNet(cat_vars, cont_vars, embedding_sizes)\n",
    "\n",
    "        # fit model with training data and evaluate using validation data\n",
    "        lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto', cooldown=0, min_lr=0.0001)\n",
    "        checkpoint = ModelCheckpoint('best_model_weights.hdf5',monitor='val_loss',save_best_only=True)\n",
    "        early_stopping_monitor = EarlyStopping(patience=20)\n",
    "\n",
    "        model.fit(x=X_arr, y=y, validation_data=(X_val_arr, y_val), epochs=100,\n",
    "                  callbacks=[lr_reducer, checkpoint, early_stopping_monitor], verbose=0)\n",
    "        pred = model.predict(x=X_test_arr)*y_max\n",
    "        rmse_list.append(rmse(pred,y_test*y_max))\n",
    "    return np.mean(np.array(rmse_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/meiliu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/meiliu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/meiliu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/meiliu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/meiliu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/meiliu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/meiliu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/meiliu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/meiliu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/meiliu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/meiliu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/meiliu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/meiliu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/meiliu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/meiliu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/meiliu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/meiliu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/meiliu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/meiliu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/meiliu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/meiliu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/meiliu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/meiliu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/meiliu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/meiliu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/meiliu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/meiliu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/meiliu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/meiliu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/meiliu/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg RMSE across 30 iterations:  16.880052797800467\n"
     ]
    }
   ],
   "source": [
    "avg_rmse2 = EmbedNetFit(30)\n",
    "print('Avg RMSE across 30 iterations: ', avg_rmse2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE is higher than the simple price based LSTM model, but across most runs (not shown above, but just what I observed from running the model multiple times), there is a clear decline in the predicted price from the first prediction date to the last prediction date. The model is capturing the decline in price, which starkly differs from previous models (eg XGBoost, LSTM). However, the prediction is too high. If some recurrent component, where the prediction can use sequential information from previous samples as a baseline for the price, is combined with the sequential model with embedding, the prediction might be able to capture both the decline and the general price level. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Future Steps: Incorporate categorical embedding into RNN or LSTM model with multiple continuous features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
